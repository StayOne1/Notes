
--- 

title: Hadoop 
tags:  
  - 大数据

categories:   
  - 大数据  

mathjax: true  
date: 2025-03-11  
cover:  
feature: false  

---
### 一、**Hadoop：分布式计算的"地基"**

**核心定位**：一个开源框架，用于在廉价硬件集群上**分布式存储和处理超大规模数据**（比如TB/PB级）。

#### 1. 两大核心模块：

- **HDFS（Hadoop Distributed File System）**
    
    - **作用**：把超大文件拆分成块（默认128MB/块），分散存储在多台机器的硬盘上。
        
    - **类比**：就像把一本超厚的书拆成多个章节，分给不同的人保管，每个人只负责自己那部分。
        
- **MapReduce**
    
    - **作用**：一种编程模型，用于**并行处理**分布式存储的数据。
        
    - **流程**：分两个阶段（Map和Reduce），后文会单独解释。
        

#### 2. 特点：

- **适合场景**：离线批处理（比如每天凌晨统计前一天的日志）。
    
- **缺点**：中间结果要写硬盘，速度较慢。
    
- **扩展性**：通过加机器就能扩容，适合廉价硬件。
    

---

### 二、**MapReduce：Hadoop的计算逻辑**

**核心思想**：将任务拆分成"分散处理+汇总结果"的模式。

#### 举个🌰：统计1000本书中每个单词出现的次数（经典的WordCount问题）

1. **Map阶段**（分散处理）：
    
    - 每台机器处理自己负责的那部分书，输出 `<单词, 1>` 的键值对（例如 `("apple", 1)`）。
        
    - **并行执行**：所有机器同时处理自己的数据。
        
2. **Shuffle阶段**（自动完成）：
    
    - 系统自动将相同单词的键值对**归并**到一起（例如所有 `"apple"` 会被集中到一台机器）。
        
3. **Reduce阶段**（汇总结果）：
    
    - 对同一单词的所有 `1` 求和，得到 `<单词, 总次数>`（例如 `("apple", 158)`）。
        

#### 关键点：

- **容错机制**：某个节点故障时，任务会自动转移到其他节点。
    
- **适用场景**：ETL（数据清洗）、日志分析等**批处理任务**。
    

---

### 三、**Spark：更快的"升级版"计算引擎**

**核心改进**：用**内存计算**大幅提升速度（比Hadoop MapReduce快10~100倍）。

#### 1. 关键特性：

- **内存计算**：中间结果优先存内存，减少硬盘IO（MapReduce的瓶颈所在）。
    
- **DAG引擎**：将任务拆分成有向无环图，优化执行流程。
    
- **API丰富**：支持Java/Scala/Python/R，提供更高级的操作（如SQL查询、机器学习库）。
    

#### 2. 核心抽象：**RDD（弹性分布式数据集）**

- **特点**：数据分片存储在内存中，可并行操作。
    
- **操作类型**：
    
    - **转换（Transformations）**：如 `map()`、`filter()`（生成新的RDD，但不会立即执行）。
        
    - **行动（Actions）**：如 `count()`、`collect()`（触发实际计算）。
        

#### 3. 适用场景：

- **迭代计算**：比如机器学习算法需要多次遍历数据。
    
- **流处理**：Spark Streaming可处理实时数据流。
    
- **交互式查询**：类似SQL的即时分析。
    

---

### 四、三者的关系总结

||**Hadoop**|**Spark**|**MapReduce**|
|---|---|---|---|
|**角色**|生态系统（存储+计算）|计算引擎|编程模型（属于Hadoop）|
|**存储依赖**|自带HDFS|可独立或集成HDFS|依赖HDFS|
|**速度**|较慢（硬盘IO）|快（内存计算）|慢|
|**适用场景**|离线批处理|迭代计算/实时流|离线批处理|

#### 举个实际应用例子：

- **数据仓库**：用Hadoop HDFS存原始数据 → 用Spark做清洗和分析 → 结果存回HDFS。
    
- **推荐系统**：用Spark Streaming实时处理用户点击流，同时用Spark MLlib训练模型。
    

---

### 五、总结

- **Hadoop** 是基础，适合低成本存储和简单批处理。
    
- **MapReduce** 是Hadoop的原生计算模型，但逐渐被Spark取代。
    
- **Spark** 是更快的通用计算引擎，适合复杂任务和实时场景。
    
- **实际中**：三者常结合使用（例如用HDFS存数据，用Spark做计算）。